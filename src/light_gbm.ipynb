{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "y = train[\"log_pSat_Pa\"]\n",
    "X = train.drop(columns=[\"log_pSat_Pa\"])\n",
    "\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "X = pd.get_dummies(X, columns=non_numeric_cols, drop_first=True)\n",
    "test = pd.get_dummies(test, columns=non_numeric_cols, drop_first=True)\n",
    "\n",
    "X, test = X.align(test, join='left', axis=1)\n",
    "test = test.fillna(0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 925\n",
      "[LightGBM] [Info] Number of data points in the train set: 21309, number of used features: 28\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score -5.539761\n",
      "Top 15 most important features:\n",
      "             Feature  Importance\n",
      "7          NumOfConf         918\n",
      "0                 ID         746\n",
      "8      NumOfConfUsed         692\n",
      "1                 MW         487\n",
      "3             NumOfC         354\n",
      "6     NumHBondDonors         310\n",
      "2         NumOfAtoms         288\n",
      "11  hydroxyl (alkyl)         270\n",
      "13            ketone         240\n",
      "4             NumOfO         210\n",
      "R2 Score on Validation Set: 0.7432\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse')\n",
    "\n",
    "feature_importance = lgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "most_important_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(most_important_df.head(10))\n",
    "\n",
    "y_pred = lgb_model.predict(X_val)\n",
    "print(f\"R2 Score on Validation Set: {r2_score(y_val, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the least important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least important features:\n",
      "                              Feature  Importance\n",
      "27         parentspecies_apin_toluene           0\n",
      "26  parentspecies_apin_decane_toluene           0\n",
      "29       parentspecies_decane_toluene           0\n",
      "19                  aromatic hydroxyl           0\n",
      "['parentspecies_apin_toluene', 'parentspecies_apin_decane_toluene', 'parentspecies_decane_toluene', 'aromatic hydroxyl']\n"
     ]
    }
   ],
   "source": [
    "least_important_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Display least important features\n",
    "print(\"Least important features:\")\n",
    "print(least_important_df.head(4))\n",
    "\n",
    "least_important_names = list(least_important_df['Feature'].head(4))\n",
    "print(least_important_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding out the least important features, we can re-trained the model and change the values of some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 923\n",
      "[LightGBM] [Info] Number of data points in the train set: 21309, number of used features: 27\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score -5.539761\n",
      "R2 Score on Re-Trained Validation Set: 0.7447\n"
     ]
    }
   ],
   "source": [
    "X_reduced = X.drop(columns=least_important_names)\n",
    "\n",
    "X_reduced, test = X_reduced.align(test, join='left', axis=1)\n",
    "test = test.fillna(0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reduced, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=50,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse')\n",
    "\n",
    "feature_importance = lgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "y_pred = lgb_model.predict(X_val)\n",
    "print(f\"R2 Score on Re-Trained Validation Set: {r2_score(y_val, y_pred):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the features with 0 importance and reducing them and lifting the num_leaves parameter from 31 to 50 and lowering n_estimators from 200 to 100, the R2 score went up to 0.7447, i.e. there is a rise of 0.0015. After playing around with other parameters, no improvements with the R2 score was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More hyperparameter optimization after some tries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 922\n",
      "[LightGBM] [Info] Number of data points in the train set: 21309, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -5.539761\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "R2 Score: 0.7471\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=27,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=3000,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.91,\n",
    "    max_depth=10,\n",
    "    lambda_l1=0.1,\n",
    "    subsample_for_bin=12000,\n",
    "    subsample=0.8,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=10,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgb_model.predict(X_val)\n",
    "print(f\"R2 Score: {r2_score(y_val, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a improvement of 0.7447 -> 0.7471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=3, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=3, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 922\n",
      "[LightGBM] [Info] Number of data points in the train set: 21309, number of used features: 27\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=3, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score -5.539761\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=3, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.91, subsample=0.8 will be ignored. Current value: bagging_fraction=0.91\n",
      "R2 Score: 0.7483\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=27,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=3000,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.91,\n",
    "    max_depth=10,\n",
    "    lambda_l1=0.1,\n",
    "    subsample_for_bin=12000,\n",
    "    subsample=0.8,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    bagging_freq= 3,\n",
    "    min_data_in_leaf=3\n",
    ")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='rmse',\n",
    ")\n",
    "\n",
    "y_val_pred = lgb_model.predict(X_val)\n",
    "\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added two hyperparameters: bagging_freq and min_data_in_leaf. Tried many different combinations for both and found that with bagging_freq = 3, the R2 reached the highest score.\n",
    "Improvement: 0.7479 -> 0.7483 (0.0004)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
